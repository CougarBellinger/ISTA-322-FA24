{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark workbook 2!\n",
        "\n",
        "Welcome to our second workbook on Pyspark.  The main goal of this workbook is to teach you the critical transform operations we learned using pandas but now via pyspark.  We'll also learn how to use SQL queries to wrangle data in pyspark, which is wonderful for those of you who prefer SQL to python for wrangling.\n",
        "\n",
        "There's little here that's conceptually new, but more this is just new syntax that I want you to learn."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "aeb93c07-e50c-4321-8b38-e4c84d27706a"
        },
        "id": "UjXYCNad38J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "oMNwJMyCVBgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "RNxF0f3CVI0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Spark Session\n",
        "First we create the Spark Session as before"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7f479e49-5e6a-48c3-ac2d-b8965eaf44f6"
        },
        "id": "gDBkdRxd38J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"pyspark-2\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2b11817b-a3a7-425f-a7d7-b9d4a71e28ba"
        },
        "id": "_iWOngpQ38J6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing our data\n",
        "\n",
        " We'll be using the NYC Taxi data again, but this time from only a single month (June, 2020).  Let's bring the data like we did last time:\n",
        " * Assign the URL location to an object called 'url'\n",
        " * Add that file to our spark session with `sparkContext.addFile()`\n",
        " * Create a filepath to the location of that file by using `.get()` to get the location of the file in our spark session\n",
        " * Read that file along with some options to infer the schema, assign a header, and make timestamps"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8779fecb-a601-4c9b-80f9-8184a0adf22b"
        },
        "id": "e1aguz4_38J7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create our filepath\n",
        "\n",
        "Let's start by creating our filepath.  The url is a link to the data directly on the NYC taxi authorities Amazon S3 storage.  Note that my `.get()` then uses the filename from the end of the  URL"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "114b5aaa-f15c-4039-9ae6-51943c8638a4"
        },
        "id": "fdfyq-ce38J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "url =  \"http://131.193.32.85:9000/mybucket/yellow_tripdata_2020-06.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "fp = 'file://'+SparkFiles.get(\"yellow_tripdata_2020-06.csv\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "11dfe689-4ca7-434e-91d4-a6f3b8b4e720"
        },
        "id": "Vtc6FkS938J8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import our data\n",
        "\n",
        "Now that we have filepath to the taxi data stored as 'fp', let's read it in.  We'll use `spark.read.csv()` like before.  We'll also set a few options:\n",
        "* 'header' we'll set to true so that it reads the first row as column names\n",
        "* 'inferSchema' as True so that it infers and applies datatypes to the columns\n",
        "* 'timestampFormat' we'll use to take the timestamps in the two columns and turn them into datetimes vs. strings\n",
        "\n",
        "The first two you've seen in the last lesson.  Let's add in the 'timestampFormat' option.  You add it in like any other option, but instead of staying `True` to indicate that you want to apply that option, you instead give it the format of the strings that contain the date and time info.  In this case the format is something like '2020-06-22 08:58:06' for a ride that occured on June 22nd at 8:58:06am.  Thus, the second argument needs to be that generalized format of \"yyyy-MM-dd HH:mm:ss\".  It's worth pointing out that \"M\" denotes the month and \"m\" denotes the minute. <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\" target=\"_blank\">You can see a full list of datetime formatting here</a>"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "01cfa33e-54f6-4ea5-b21b-408ce0c5cdd6"
        },
        "id": "ra41VA6Q38J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in our csv with the options applied\n",
        "rides_df = (spark.read\n",
        "            .option('header', True)\n",
        "            .option('inferSchema', True)\n",
        "            .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\n",
        "            .csv(fp))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a1be1d10-3c45-4965-8b49-ad42a7ff2f1b"
        },
        "id": "KVTd5B6138J9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the schema of 'rides_df' to see the datatypes\n",
        "\n",
        "You can see that the 'tpep_pickup_datetime' and 'tpep_dropoff_datetime' columns are listed as timestamps"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "877a4b4e-9a6f-48a4-aab1-65b5a52d162e"
        },
        "id": "a7fAARaS38J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rides_df.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b2aa5c8d-aebf-4ff9-8766-919c9b220d3e"
        },
        "id": "-twVEKL138J9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a quick look using show\n",
        "rides_df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f67c8e49-6514-4120-933f-488d25246539"
        },
        "id": "aAzYj8_f38J-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping data\n",
        "\n",
        "We learned earlier that it's common to have to drop data.  This can be either whole columns because they're unnecessary, redundant, or because the end user does not need to see them.  We also might have to drop duplicated rows.  Let's take a quick look at the pyspark ways to do these actions.  \n",
        "\n",
        "* First we're going to cut out columns.  This actually uses `.drop()` just like python.\n",
        "* Next we're going to deduplicate our data using `dropDuplicates()`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f25bd420-5033-4612-834b-100194eb443c"
        },
        "id": "GUCuJJ6D38J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping columns\n",
        "Starting with `.drop()`, let's remove the columns 'RatecodeID', 'store_and_fwd_flag', 'mta_tax', 'tolls_amount', and  'improvement_surcharge'. To do this you simply give it a series of column names inside `.drop()` This is almost identical to pandas except for that in pyspark it's not a proper list (e.g. ['column1', 'column2']) and instead just a comma separated list of names inside the function.  \n",
        "\n",
        "So the python code would be ```rides_df = rides_df.drop(['RatecodeID', 'store_and_fwd_flag', 'mta_tax', 'tolls_amount', 'improvement_surcharge'])```\n",
        "\n",
        "While the pyspark code is ```rides_df = rides_df.drop('RatecodeID', 'store_and_fwd_flag', 'mta_tax', 'tolls_amount', 'improvement_surcharge')```"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9ccca2ee-9867-4d1b-892a-1c8d21e0832a"
        },
        "id": "HiWkAeIr38J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop those columns\n",
        "rides_df = rides_df.drop('RatecodeID', 'store_and_fwd_flag', 'mta_tax', 'tolls_amount', 'improvement_surcharge')"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "df7cc8e4-9960-4a80-b0db-ce34f6c52e24"
        },
        "id": "HE00HPip38J-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "rides_df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0d839405-1892-4d38-b975-66a0b8eba924"
        },
        "id": "Y3MsbKoG38J_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping rows\n",
        "\n",
        "Let's drop some duplicated rows now.  This dataset doesn't actually have any, but for the sake of demonstrating functionality we're going to drop any row that has a duplicated pickup time.  You would apply this method any time you want to make sure you only have unique values a row.  \n",
        "\n",
        "First we'll check the number of rows in the data.  This uses `.count()` vs. `.shape()` in pandas world.  \n",
        "\n",
        "Then we'll apply the `dropDuplicates()` method from pyspark.  As you can imagine, this works just like `drop_duplicates()` in pandas.  In this case, you do feed it an actual list of values of what you want to drop. As we want to drop any row that has the exact same pickup time, we'll insert '[tpep_pickup_datetime]' inside the method."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e57cf147-94c6-445e-a1ac-5b4953ef360f"
        },
        "id": "isMkz0m-38J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, how many rows do we have?\n",
        "rides_df.count()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d69df55a-9d4d-40da-ba4f-7534778106ba"
        },
        "id": "tPlJce6d38J_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's dropDuplicates\n",
        "rides_df= rides_df.dropDuplicates(['tpep_pickup_datetime'])"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f4ac5450-e70a-42da-baa2-21e5b2c455ab"
        },
        "id": "n1qqtPIo38KA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Now how many rows?\n",
        "rides_df.count()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a1f0c5d1-b6cd-497d-8659-481e7939c799"
        },
        "id": "9hycPohC38KA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting and filtering data\n",
        "\n",
        "Pyspark makes it really easy to also filter data based on conditions.  There are two identical functions, `filter()` and `where()`, which both allow you to specify a column name and condition within the column to filter data.  The `where()` exists for those people coming from a SQL background.  \n",
        "\n",
        "Selecting columns and values is also easy using `.select()`\n",
        "\n",
        "Let's start by filtering all rides that are above average in length.  We'll this as follows:\n",
        "* Make an 'avg_dist' variable where we select the 'trip_distance' column and take the average.  \n",
        "* Filter using `where()` to filter out only rows that have an above average distance."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "056a8bc9-641c-4da1-9836-18e56cf4f39e"
        },
        "id": "HPqtyvPT38KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making average distance variable\n",
        "\n",
        "Using pandas you'd get the average value of a column using code like this `df['col_name'].mean()`. Pyspark uses a bit different syntax in that you apply the mathematical function inside the `.select()` you use to select your column.  So the equivalent pyspark code would be `df.select(avg('col_name'))`.\n",
        "\n",
        "The other difference is that pyspark returns a dataframe, so you need to select just the first value of it if you want to store it as an object and use later.  Watch."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "db820ef6-14c4-4205-9201-a5dde121d822"
        },
        "id": "Zc9j4zxw38KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bulk import all the functions from pyspark.sql.functions\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cfd6b229-e819-4e2e-b8b3-c3da94f99e43"
        },
        "id": "3c-ib-Ph38KB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Note how getting the average trip distance returns a dataframe\n",
        "rides_df.select(avg('trip_distance'))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2b26a27f-dcfb-494c-8bd6-d26253cddf2b"
        },
        "id": "bs132sc538KB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# If you use .show() you can see this more clearly.\n",
        "rides_df.select(avg('trip_distance')).show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3fbd237f-f845-43dc-9fe7-70e3a27ec698"
        },
        "id": "GDsaFPKZ38KB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extract just that first value by applying the `.first()` method and then selecting the 0th position"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a48c088b-dceb-4a4e-b0b8-f4179e010aed"
        },
        "id": "gvPogfhi38KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can select just that first value\n",
        "rides_df.select(avg('trip_distance')).first()[0]"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "da814444-637f-4ac8-8f8f-56f4d090fc92"
        },
        "id": "iXRny7_s38KC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Make your avg_dist object\n",
        "avg_dist = rides_df.select(avg('trip_distance')).first()[0]"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "70c9d90e-5d54-492c-ab66-1a15f5dbec7b"
        },
        "id": "5AXMLhRp38KC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using where to filter\n",
        "Now we can use `where()` to select only rows where the trip distance is greater than that average distance.  We'll make this into a dataframe called 'long_df'"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6471221b-0dbe-4c75-8751-017670b43483"
        },
        "id": "0fffPCu338KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_df = rides_df.where(col('trip_distance') >= 4)\n",
        "long_df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b8c42a54-b419-405d-b63d-f682540e7b15"
        },
        "id": "dKHblwfD38KC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was a simple example, but you can use similar functionality of min, max, count to then use SQL like filtering via `.where()`.\n",
        "\n",
        "For example, let's do a filter that is making sure we're only having rides that are greater than 0 miles but less than 15.  These values are reasonable and unlikely to be errors."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7b479afa-83f3-4730-b5a6-5735242d52ba"
        },
        "id": "AhBy5vrp38KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_trip_dist = rides_df.select(min('trip_distance')).first()[0]\n",
        "max_trip_dist = rides_df.select(max('trip_distance')).first()[0]\n",
        "\n",
        "# Now filter based on both\n",
        "rides_df.where( (col('trip_distance') > min_trip_dist) & (col('trip_distance') < max_trip_dist) ).show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ba1c5b18-36bc-435a-af60-456eee7b3543"
        },
        "id": "zIYgBV9j38KC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Go and swap out .where for .filter just to show that  you get the same results!\n",
        "..."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2bc14a3c-1b97-48de-9fcc-fcf661f8eef0"
        },
        "id": "XbFoS4ow38KD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark and SQL queries\n",
        "\n",
        "One great thing about pyspark is that you can actually work on your dataframe using straight SQL syntax! All you need to do is to turn the dataframe into a table and then you can run queries.  \n",
        "\n",
        "To make the table you use the code `df_name.createGlobalTempView('what_to_call_table')`.  This takes the dataframe and creates a SQL table with the name given in `createGlobalTempView`.\n",
        "\n",
        "After that, you can run queries using the `.sql()` function.  \n",
        "\n",
        "Let's create a table called 'rides' from our 'rides_df' dataframe."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "742c1558-fe2a-4620-9f69-3363da99aa7b"
        },
        "id": "f-GploIY38KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.dropGlobalTempView(\"rides\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8ce83741-e25b-41ff-99c9-3e6b65d31de8"
        },
        "id": "5UO6Fzmb38KD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# create 'rides' table\n",
        "rides_df.createGlobalTempView(\"rides\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4b610aed-56bd-45d5-8d4f-98ebe65ca290"
        },
        "id": "ptp_CfP938KE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run a SQL query\n",
        "\n",
        "Now let's run a simple sql query that gets all rides with only one passenger and just the trip_distance column.  You write this as a normal SQL query with one exception... your FROM statement needs to reference 'global_temp', which is the global temporary environment that the table is stored.  So in this case you'll write `FROM global_temp.rides` as rides is the name of the table."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ae03a1dd-d070-46a8-bf8f-25f044ec9538"
        },
        "id": "0PPHFnb738KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sq = \"\"\"SELECT trip_distance FROM global_temp.rides\n",
        "          WHERE passenger_count = 1\n",
        "          LIMIT 5\"\"\"\n",
        "spark.sql(sq).show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dc5ae857-ce26-4e44-a117-bac7dacea06c"
        },
        "id": "obtL8dWb38KF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can do a slightly more complicated query.  We'll extract the hour from our datetime using `date_part()` so that we can count up the number of rides in each hour using GROUP BY.  Note that my COUNT is just calling 'VendorID'.  This is because COUNT just counts the number of rows in a column, and since each trip is a row it doesn't matter which column we count from."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7b4c57c8-771f-4a0d-8749-26150c4e0faf"
        },
        "id": "9cwymDwG38KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sq = \"\"\"SELECT date_part('hour', tpep_pickup_datetime) as hour, COUNT(VendorID) as number_rides FROM global_temp.rides\n",
        "          GROUP BY hour\n",
        "          ORDER BY hour\"\"\"\n",
        "spark.sql(sq).show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dfeb3c45-e77e-4ccc-a0f0-526d1863e713"
        },
        "id": "83CAJooy38KF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping up\n",
        "\n",
        "That's it for pyspark in this class!  As with anything, it's by no means exhaustive.  But hopefully you can see the connection between pyspark functionality and the regular pandas and SQL ideas and tools we've learned so far.   \n",
        "\n",
        "One last thing I want to mention.  Just because you can do it in pyspark doesn't mean you should.  Given the distributed nature of spark, some operations could be slower on pyspark vs. just python.  This could be the case if your data aren't actually large, in which case all the moving of data around across the cluster could actually slow things down!  \n",
        "\n",
        "Another thing worth mentioning is that you might not want to be using pyspark as a tool for data exploration.  If you're working with super big data, it might be worth sampling a subset of data to bring locally, exploring and developing your transformations there, then going and running it all on the full data and cluster.  Like anything, these are tools and your situation will dictate when to apply what tool!"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dbb933ec-12c7-408b-8394-59274ebb9c9a"
        },
        "id": "2z1yRrrN38KF"
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "ISTA_322_Databricks_Wb2",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 347964148883577
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}