{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Pyspark\n",
        "\n",
        "Welcome to the lesson on working with pyspark ."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f62a44d2-f577-486a-ae43-27cafac260b6"
        },
        "id": "hYBcBeWNMgL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries\n",
        "First we have to install pyspark.\n",
        "\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9a57485b-ee32-4b82-97bd-19db34a15ddc"
        },
        "id": "r0LlTzRLMgL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "edIffpUuSTNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we  import pyspark"
      ],
      "metadata": {
        "id": "Yrbi3j51SNRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1c7ecf73-2de3-4e09-8cb8-842de59ab9f1"
        },
        "id": "-Jjz2xtQMgL_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Spark Session\n",
        "\n",
        "In order to actually use spark you need to create something called a `Spark Session`.  Your spark session is what creates the context and configuration for spark to run on and allows spark to access all the functionality (e.g. make dataframes, run SQL queries, etc).\n",
        "\n",
        "if you're going to work locally or on any other configuration you need to  create your session.  You can also start setting custom configuration options here as well.  For example if you wanted to work locally and use four cores of your CPU (assuming you have four), you could set the `.master()` option to `.master(\"local[4]\")`\n",
        "\n",
        "For now, we'll just call `SparkSession.builder` and then `getOrCreate()` to tell spark to build up a session for us to use.  It'll automatically default to standard options based on the configuration of the machine."
      ],
      "metadata": {
        "id": "W6NsvZ6ASD0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"intro_pyspark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d6d464b0-b71d-45dc-986f-871304403aae"
        },
        "id": "D1WiffPRMgMB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing data\n",
        "\n",
        "To get files into your environment and accessible to spark you need to provide a path to where the file is located in this environment.  To do this you add the file to your spark environment with `sparkContext.addFile()` and feed it the URL inside the `addFile()` function.  \n",
        "\n",
        "We'll need to import the `SparkFiles` function from pyspark as well."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cce2b4a4-9987-4984-8ff9-dae041263a1c"
        },
        "id": "g2uNp9PnMgMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "url = 'http://131.193.32.85:9000/mybucket/covid_daily_cases.csv'\n",
        "spark.sparkContext.addFile(url)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "13f28e39-e844-4dde-83d1-5d963b18d256"
        },
        "id": "W-AydNr9MgMC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once that's done now you can read it in using the local filepath.\n",
        "\n",
        "Using `SparkFiles` and `.get()` we can get the filepath to our data called 'covid_daily_cases.csv'"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6d128a62-e514-40ba-9a32-2d52ba049041"
        },
        "id": "Le-UeZ5OMgMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SparkFiles.get(\"covid_daily_cases.csv\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0e36679f-27da-4d04-8cc7-430f7127af6a"
        },
        "id": "UrejvWaTMgMC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we need to provide the full path, so we must add 'file://' before that path to the file."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "82453d53-68e4-41cd-8617-a26fcdd641a7"
        },
        "id": "XMqRlB6rMgMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp = 'file://'+SparkFiles.get(\"covid_daily_cases.csv\")\n",
        "fp"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "52acaf6d-ad3d-48eb-b774-bbbb995b91fc"
        },
        "id": "WuOXKK4-MgMD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!  We have a filepath to our data.  Reading data through pyspark is similar to pandas.  We first call our spark session.  Then the `.read` function, followed by the file type function.  In this case `.csv()`.  Inside `.csv()` you can feed it the filepath"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4e8bd7e5-372f-46a7-bd16-db215872e4ff"
        },
        "id": "H77OUuU5MgMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid = spark.read.csv(fp)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "efd84542-5906-454e-9630-83da3da6bcf3"
        },
        "id": "SjDOuIZCMgMD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a quick look at our data by calling the `.show()` method on our covid data.  `.show()` Kicks back a nice neatly formatted view of your data."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f8768e65-96c1-4c3f-b4d6-adb8b59e4082"
        },
        "id": "kJD3mr9pMgME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use .show()() to look at your data\n",
        "covid.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d86e59d8-e0f9-4163-b56b-526827e1c5e0"
        },
        "id": "rXDizf-5MgME"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modifying your import\n",
        "\n",
        "Spark has some clear differences in how it imports data.  The big one you can probably see from our `.show()` is that unlike pandas, pyspark doesn't by default import the first row of your dataframe as the column names.  \n",
        "\n",
        "You can modify that behavior along with a host of other import behaviors by adding in a `.option()` call.  Inside you can feed it a specific option you want to modify.  In this case we'll specify `'header, True'`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b37eb87f-19e2-4726-86e1-a3c88cd0808c"
        },
        "id": "zg2SP2iHMgMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid = (spark.read\n",
        "         .option('header', True)\n",
        "         .csv(fp)\n",
        "        )\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "79fc72cb-e7e1-482f-a58e-49d3dc4c5a3d"
        },
        "id": "AG24iaS2MgMF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other options such as specifying the delimiter.  Obviously we're using a comma, but you might have a file that needs to be separated by a different symbol."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "eefad003-12eb-4407-be6f-d96d42ad9b1a"
        },
        "id": "DUVeVh2YMgMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid = (spark.read\n",
        "         .option('header', True)\n",
        "         .option('delimiter', ',')\n",
        "         .csv(fp)\n",
        "        )"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3793833e-f4b4-4ab2-a8f9-ce92f0aab6cd"
        },
        "id": "7KphR6kvMgMF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Printing and altering schema\n",
        "\n",
        "Pyspark readily works with unstructured data.  We're going to hold off on that, but one thing that's worth noting here is that you can apply a schema to your data on import.  This is important for working with nested data.  **But**, it's also important because pyspark doesn't infer datatypes when importing data.  This means that the default read is to have everything be strings. Let's take a look at the schema using the `printSchema()` method."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "125fed40-0021-453d-b47c-a2d762d7df40"
        },
        "id": "CshwbQNfMgMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use printSchema()\n",
        "covid.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e8fd05b9-e462-47e1-9bb6-6e5f38cddba0"
        },
        "id": "KPKHGqATMgMG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "So above we can see that all our columns are at the same level.  But also that they're all strings and null values are allowed.  But should they all be strings?  Of course not!  Many of the columns are just numeric.  We can give our read another option to infer what the schema and datatypes should be.  Just toss in `.option('inferSchema', True)`"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8fbd0d04-4f8a-47bd-94eb-b9599bf55731"
        },
        "id": "_W3mi6JMMgMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid = (spark.read\n",
        "         .option('header', True)\n",
        "         .option('delimiter', ',')\n",
        "         .option('inferSchema', True)\n",
        "         .csv(fp)\n",
        "        )"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c9109d60-5fed-42cb-b23c-ac9eecf84294"
        },
        "id": "gYO8pbpBMgMG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at our schema below we can see that now the numeric columns are all considered as integers.  We'll convert those next."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4c3f8188-534a-4b1d-94ce-3f9615681d4f"
        },
        "id": "JPEMNfukMgMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "42032f4e-bcb5-47dd-8226-51eac504a063"
        },
        "id": "KcG8tMcEMgMG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datatype conversions in pyspark\n",
        "\n",
        "Datatype conversions in spark are conceptually straightforward, but the syntax is a bit different.  In python you'd use a format as follows to convert a column to an integer:\n",
        "```\n",
        "df['new_column'] = df['column_to_convert'].astype(int)\n",
        "```\n",
        "\n",
        "But in pyspark you it differs a bit.  Specifically, you'll use the `.withColumn()` function to apply a datatype to an existing column.  The generalized format is:\n",
        "```\n",
        "df = df.withColumn('new_column', col('column_to_convert').cast(dataType()))\n",
        "```\n",
        "\n",
        "The first argument within `withColumn()` is just the column name you want to asign the output of modified column.  This could be something new, or it could overwrite an the column you modified... for example if you just wanted change a datatype of a column you'd overwrite.  \n",
        "\n",
        "The second argument then selects a column.  You select columns a bit differently in pyspark, instead using `col()` with the column name inside.  `.cast()` then, well ,casts that column as a new datatype.  The datatype you specify inside `.cast()`.  \n",
        "\n",
        "A third thing to note.  You actually import your datatype methods.  So 'IntegerType', 'StringType', etc.  \n",
        "\n",
        "The last thing to note.  In python you'd create a new column by calling `df['new_name']` on the left side of the equals.  When using pyspark's `withColumn()` function you only need to put the dataframe to the left of the equals as it understands that you want to make a new column inside that dataframe and not overwrite the whole thing.\n",
        "\n",
        "Let's go convert 'county_code' to a string and then make a new column called 'date_dt' that's a datetime version of our 'date' column."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f10a0e07-6786-4f7a-aa57-3d8310b16c29"
        },
        "id": "KOWmmKXoMgMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our needed functions\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType, StringType, DateType, TimestampType\n",
        "# Also need to modify a setting to deal with a later date conversion - don't need to know this!\n",
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c5eb5d5d-4100-409f-bfe7-8e7c72d9ce53"
        },
        "id": "1jVykpCuMgMH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, so we'll start by converting the 'county_code' column back to a string.  \n",
        "\n",
        "The code below essentially says \"make a new column called 'country_code' using the existing column 'country_code' cast as a string datatype\""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9f2bbba1-7967-4232-bb68-39adee3e890e"
        },
        "id": "swRQX61_MgMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert\n",
        "covid = covid.withColumn('county_code', col('county_code').cast(StringType()))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8f990db5-b042-4cc1-9cd0-bb89dd9f2171"
        },
        "id": "rlJtbkjDMgMH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check schema to see datatypes\n",
        "covid.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3ff92f5b-e031-41ad-8088-f1294d8475a0"
        },
        "id": "qeQv5J2XMgMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now let's make a new column called 'date_dt' that converts our date column from a string to a date.  \n",
        "\n",
        "The syntax is slightly different in that you call `col()` inside a `to_date()` function followed by the date format you want to convert.  Our dates are like 01/28/2020, so the format is 'mm/dd/yyyy'"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "18c9d0e1-1e97-4237-835a-68835031a603"
        },
        "id": "y98c6frFMgMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid = covid.withColumn('date_dt', to_date(col('date'), 'mm/dd/yyyy'))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f0d3a416-9f83-4825-9e47-7284db81e7eb"
        },
        "id": "1fOra5wXMgMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# check data\n",
        "covid.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4196ca17-da20-4b2d-bc45-0ce99b2b2802"
        },
        "id": "Tuj16LB9MgMJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other transforms in pyspark\n",
        "\n",
        "Let's just look at some quick transforms.  \n",
        "\n",
        "* We'll make a new column of the ratio of covid cases to deaths by.  This again just uses `withColumn()`.\n",
        "* We'll do a groupby so you can see how similar that is!"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8e48732f-d60e-46a9-9d4e-f9c7431c30ad"
        },
        "id": "fPWZBnjYMgMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by making a column of the ratio of deaths to total cases, so dividing the number of deaths by total cases.  Let's assign to a new column called 'case_death_ratio'.\n",
        "\n",
        "Again, the first argument in `withColumn` is the column name of the result, second is just the math!"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "91e5032d-ceff-4e9a-a7a7-21c628560a7e"
        },
        "id": "FkS2IGF_MgMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make 'case_death_ratio' column\n",
        "covid = covid.withColumn('case_death_ratio', col('daily_deaths')/col('daily_cases') )"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "57c31097-2724-4960-b24e-2d5c3a313775"
        },
        "id": "qbU3tO6eMgMK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "covid.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8916da56-733b-4cc6-9ce4-56a9ece94d5b"
        },
        "id": "uaRkwkrWMgMK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groupby operations work *very* similar to straight python.  The format is still `df.groupBy('column_to_group').agg(math)`.  Couple things to note:\n",
        "* pyspark uses camelCase, so it's `groupBy` not `groupby`\n",
        "* `.agg()` works a bit differently.  Instead of a dictionary of the column you want to do math on as the key and math function as the value, you instead use a format like `.agg(math_function('column_to_do_math_on'))`\n",
        "\n",
        "Let's go and count the total number of deaths by state, by county."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b12edd0b-cdc3-4c31-a89b-91742a8c7178"
        },
        "id": "MPjVhlgtMgMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make covid_grouped where we group by state and county and then sum up the number of deaths\n",
        "covid_grouped = covid.groupBy('state', 'county').agg(sum('daily_deaths'))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "45566df2-b620-4f63-a444-3b3dc0cc7305"
        },
        "id": "K7SkR-NlMgMK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "covid_grouped.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "38f1c5ff-b811-434b-876b-26a8a954f692"
        },
        "id": "ZBKmp2QTMgMK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping up\n",
        "\n",
        "This lesson was meant to be a short primer to pyspark and obviously not an exhaustive overview.  Hopefully you can see how syntax makes it easy to do all the same things that you would in python."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "51c4a105-774d-45c5-907e-76ec15ea57d1"
        },
        "id": "QsZ-x99kMgML"
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "ISTA_322_Databricks_intro",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 77692458070605
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}